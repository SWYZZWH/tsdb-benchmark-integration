## 设计

本项目是`tsdb-benchmark`项目的衍生项目，使用 `pytest` 集成测试框架，编写测试用例以比较某些时序数据库在仅某一写入压力特征不同，其余写入压力特征相同时的性能差异，并支持压力测试的有效性校验。

`tsdb-benchmark`项目是支持测试多种时序数据库在不同压力特征下写入性能的压测工具。包含一个http server，支持通过 url 请求启停指定数据库的写入压力测试，并通过请求参数配置写入压力的特征，还支持查询压力测试的报告。

时序数据库的写入性能与写入压力的特征关系很大，写入 qps 一致的情况下，线的数目很大时 cpu 占用往往明显上升（一台主机持续汇报某个指标，这些数据点形成一条线）。同时，不同时序数据库在不同写入压力特征下的好坏存在差异，如某个时序数据库可能在线的数目少时表现优于其他时序数据库，尽管在线的数目大时表现不如其他数据库。对于使用场景中线的数目少的用户来说，这是非常有价值的信息。因此，有必要设计一系列写入压力特征不同的实验来比较时序数据库的表现。

每个实验都需要对多种指定的数据库（targets），改变一个写入压力配置项（variant），其余配置项都设置为一个固定值（invariants）。这是一个二重循环，对每个target，每个variant的取值都要调用一次`tsdb-benchmark`运行一次压力测试，生成一份压力测试报告。

每个实验中虽然 targets、 variant、invariants 都不相同，但实验的流程都相同。每个实验对应一个或多个配置文件，targets、 variant、invariants 都在配置文件中指定。所有实验共享一套流程（workflow），即载入所有的配置文件，根据每个配置文件，生成调用`tsdb-benchmark`的 `url`请求并调用，回收记录每个压力测试执行的情况，最终汇总生成实验结果。有时，还需要检查压力测试是否有效——产生的数据点是否真正写入了数据库而没有被丢弃。这些额外流程被做成了可插拔的插件，是否启用可以在配置文件中指定。由于需要回收汇总压测结果，以及验证压测有效性，这种端到端的控制其实就是`tsdb-benchmark`项目的集成测试，使用集成测试框架实现这一功能非常方便，而且有一定的灵活性与可拓展性。



## 项目结构

`tests`目录下存放所有的测试用例，这些测试用例被划分进多个子模块，子模块中的`test_xxx.py`即为单个测试用例。测试用例中的测试函数将声明需要哪些配置文件并载入，以这些配置文件为参数，调用`utils`中的`workflow`实际运行`benchmark`。

`configs`目录下存放所有配置文件，`configs`目录内应与`tests`目录内保持相同的结构。所有配置文件都必须遵循一致的格式，具体格式见 `配置文件规范`。

`utils`目录下包含

- `tsdb-benchmark`的 client，封装了`tsdb-benchmark`提供的 apis
- `targets`：每个数据库都要实现`TargetInterface`接口，包含`query_total_metrics`等方法，以便验证压测有效性
- `workflow`：所有测试用例共享一套流程，用户也可以自定义 `workflow`


## 如何编写新的测试用例

根据最后需要获得哪些报告，决定有哪些测试用例。增加新的测试用例时，先想好需要什么样的报告。

每个配置文件最终对应产生一份测试报告，是测试的最小单位。

测试用例的最小单位是function，给每个测试function传入一个配置文件列表。在每个function中，可以遍历配置文件列表，生成多份报告，不过并不推荐。
推荐的方式是配置文件列表中只有一个配置文件。



## 配置文件规范

所有配置文件都要遵循相同的格式

每个配置文件包括：

- targets ：本测试包含的数据库，以及每个数据库的特定配置项
- variant：本测试的自变量，为一个值列表
- invariants：本测试的不变量
- criterion：判断本测试是否有效的判据，如 `max-metrics-missing-rate` 即设定一个最大丢点率，如果丢点率超过这个最大丢点率则本次测试视为无效
- server：测试工具的配置项，如地址、端口等

具体可以参考 configs/otel/missing_points.yaml 的写法